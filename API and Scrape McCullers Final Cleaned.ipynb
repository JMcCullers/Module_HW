{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADS 509 Module 1: APIs and Web Scraping\n",
    "\n",
    "This notebook has three parts. In the first part you will pull data from the Twitter API. In the second, you will scrape lyrics from AZLyrics.com. In the last part, you'll run code that verifies the completeness of your data pull. \n",
    "\n",
    "For this assignment you have chosen two musical artists who have at least 100,000 Twitter followers and 20 songs with lyrics on AZLyrics.com. In this part of the assignment we pull the some of the user information for the followers of your artist and store them in text files. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter API Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the twitter section\n",
    "import tweepy\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# for the lyrics scrape section\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need bring in our API keys. Since API keys should be kept secret, we'll keep them in a file called `api_keys.py`. This file should be stored in the directory where you store this notebook. The example file is provided for you on Blackboard. The example has API keys that are _not_ functional, so you'll need to get Twitter credentials and replace the placeholder keys. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_keys import api_key, api_key_secret, access_token, access_token_secret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Host api.twitter.com\n"
     ]
    }
   ],
   "source": [
    "auth = tweepy.AppAuthHandler(api_key, api_key_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "\n",
    "print ('API Host', api.host)\n",
    "#print ('API Version', api.api_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the API\n",
    "\n",
    "The Twitter APIs are quite rich. Let's play around with some of the features before we dive into this section of the assignment. For our testing, it's convenient to have a small data set to play with. We will seed the code with the handle of John Chandler, one of the instructors in this course. His handle is `@37chandler`. Feel free to use a different handle if you would like to look at someone else's data. \n",
    "\n",
    "We will write code to explore a few aspects of the API: \n",
    "\n",
    "1. Pull all the follower IDs for @katymck.\n",
    "1. Explore the user object, which gives us information about Twitter users. \n",
    "1. Pull some user objects for the followers. \n",
    "1. Pull the last few tweets by @katymck.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the first five follower ids for 37chandler out of the 189 total.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[257285645, 1469785454576820225, 1181131341687066624, 257686741, 2306579816]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I had to change api.followers_ids to api.get_followers_ids since it returned the error that it had no followers_ids??\n",
    "\n",
    "handle = \"37chandler\"\n",
    "\n",
    "followers = []\n",
    "\n",
    "for page in tweepy.Cursor(api.get_follower_ids,\n",
    "                          # This is how we will get around the issue of not being able to grab all ids at once\n",
    "                          # Once the rate limit is hit, we will be  that we must wait 15 mins (900 secs)\n",
    "#                           wait_on_rate_limit=True, \n",
    "#                           wait_on_rate_limit_notify=True, \n",
    "#                           compression=True,\n",
    "                          screen_name=handle).pages():\n",
    "\n",
    "    # The page variable comes back as a list, so we have to use .extend rather than .append\n",
    "    followers.extend(page)\n",
    "        \n",
    "        \n",
    "print(f\"Here are the first five follower ids for {handle} out of the {len(followers)} total.\")\n",
    "followers[:5]\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the follower IDs, which are unique numbers identifying the user, but we'd like to get some more information on these users. Twitter allows us to pull \"fully hydrated user objects\", which is a fancy way of saying \"all the information about the user\". Let's look at user object for our starting handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 8813602, 'id_str': '8813602', 'name': 'Stacey Burns', 'screen_name': 'WentRogue', 'location': 'Minneapolis Witch District', 'profile_location': None, 'description': 'Once inspired the NYT Science Times to \"try harder.\" And now I tweet about my cats. Cis; she/her. I used to be someone’s big sister. I am still not OK.', 'url': 'https://t.co/CgQ2gGvTks', 'entities': {'url': {'urls': [{'url': 'https://t.co/CgQ2gGvTks', 'expanded_url': 'https://unrestrictmn.org/petition-reproductive-freedom/?ms=SBtwitter', 'display_url': 'unrestrictmn.org/petition-repro…', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 10554, 'friends_count': 4453, 'listed_count': 278, 'created_at': 'Tue Sep 11 16:25:31 +0000 2007', 'favourites_count': 141964, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': True, 'statuses_count': 184673, 'lang': None, 'status': {'created_at': 'Mon May 16 21:42:28 +0000 2022', 'id': 1526317181767667717, 'id_str': '1526317181767667717', 'text': '@chris_steller What a photo!!!', 'truncated': False, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'chris_steller', 'name': 'Chris Steller', 'id': 14812037, 'id_str': '14812037', 'indices': [0, 14]}], 'urls': []}, 'source': '<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>', 'in_reply_to_status_id': 1526316858340782080, 'in_reply_to_status_id_str': '1526316858340782080', 'in_reply_to_user_id': 14812037, 'in_reply_to_user_id_str': '14812037', 'in_reply_to_screen_name': 'chris_steller', 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 0, 'favorite_count': 2, 'favorited': False, 'retweeted': False, 'lang': 'en'}, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'E9E8EB', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme5/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme5/bg.gif', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1511695479217786886/BkdOZPwV_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1511695479217786886/BkdOZPwV_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/8813602/1618767093', 'profile_link_color': 'FF0000', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': 'E9E8EB', 'profile_text_color': '0C3E53', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none', 'withheld_in_countries': []}\n"
     ]
    }
   ],
   "source": [
    "user = api.get_user(user_id=8813602) \n",
    "print(user._json)\n",
    "\n",
    "# Section Complete\n",
    "# I needed to do user_id = because it no longer displays the users, it returns an error that it expects 1 but 2 were provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 individual fields are being returned in the _json portion of the user object\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(user._json.keys())} individual fields are being returned in the _json portion of the user object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a few questions for you about the user object.\n",
    "\n",
    "Q: How many fields are being returned in the \\_json portion of the user object? \n",
    "\n",
    "A: There are 45 fields being returned, they can be seen above for the user I chose from the list of followers. \n",
    "\n",
    "---\n",
    "\n",
    "Q: Are any of the fields within the user object non-scaler? TK correct term\n",
    "\n",
    "A: I am not sure what TK correct term means, but there appears to be non-scaler fields that are not integers or other scaler fields. This can be seen in the above listing. \n",
    "\n",
    "---\n",
    "\n",
    "Q: How many friends, followers, favorites, and statuses does this user have? \n",
    "\n",
    "A: Friends: 4434, Followers: 10523, Favorites: 141585, Statuses: 184339 **This section seems to keep changing when I run each day. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can map the follower IDs onto screen names by accessing the screen_name key within the user object. Modify the code below to also print out how many people the follower is following and how many followers they have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37chandler is followed by HicSvntDraconez\n",
      "HicSvntDraconez is following 1546 people\n",
      "HicSvntDraconez has 33 followers\n",
      "\n",
      "37chandler is followed by JohnOCo70713197\n",
      "JohnOCo70713197 is following 8 people\n",
      "JohnOCo70713197 has 1 followers\n",
      "\n",
      "37chandler is followed by CodeGradeCom\n",
      "CodeGradeCom is following 2711 people\n",
      "CodeGradeCom has 388 followers\n",
      "\n",
      "37chandler is followed by cleverhoods\n",
      "cleverhoods is following 2774 people\n",
      "cleverhoods has 3371 followers\n",
      "\n",
      "37chandler is followed by PaulNaish78\n",
      "PaulNaish78 is following 19266 people\n",
      "PaulNaish78 has 19524 followers\n",
      "\n",
      "37chandler is followed by mplsFietser\n",
      "mplsFietser is following 2651 people\n",
      "mplsFietser has 2783 followers\n",
      "\n",
      "37chandler is followed by echallstrom\n",
      "echallstrom is following 457 people\n",
      "echallstrom has 304 followers\n",
      "\n",
      "37chandler is followed by byler_t117\n",
      "byler_t117 is following 440 people\n",
      "byler_t117 has 48 followers\n",
      "\n",
      "37chandler is followed by Community_Owner\n",
      "Community_Owner is following 47 people\n",
      "Community_Owner has 31 followers\n",
      "\n",
      "37chandler is followed by DeepakC64237257\n",
      "DeepakC64237257 is following 574 people\n",
      "DeepakC64237257 has 31 followers\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# I had to add in user_id= to be able to run properly, update from tweepy\n",
    "\n",
    "ids_to_lookup = followers[:10]\n",
    "\n",
    "for user_obj in api.lookup_users(user_id=ids_to_lookup) :\n",
    "    print(f\"{handle} is followed by {user_obj.screen_name}\")\n",
    "    \n",
    "    # Add code here to print out friends and followers of `handle`\n",
    "    print(f\"{user_obj.screen_name} is following {user_obj._json['friends_count']} people\")\n",
    "    print(f\"{user_obj.screen_name} has {user_obj._json['followers_count']} followers\\n\")\n",
    "    \n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although you won't need it for this assignment, individual tweets (called \"statuses\" in the API) can be a rich source of text-based data. To illustrate the concepts, let's look at the last few tweets for this user. You are encouraged to explore the `status` object and marvel in the richness of the data that is available. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tweet was tweeted at 2022-05-13 23:46:46+00:00.\n",
      "The original tweet has been retweeted 5 times.\n",
      "RT @johnhollinger: @NateSilver538 Atlanta still leads the nation in \"further West than you think\"\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-13 12:03:18+00:00.\n",
      "The original tweet has been retweeted 1478 times.\n",
      "RT @tomscocca: It was helpful to talk with @pareene about the experience of turning away from one of the most comfortable default beliefs o…\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-12 14:41:18+00:00.\n",
      "The original tweet has been retweeted 187 times.\n",
      "RT @JamesTateHill: I Actually Thought You Had Dropped the Class: A Memoir of Your Final Grade\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-12 12:19:43+00:00.\n",
      "The original tweet has been retweeted 0 times.\n",
      "@BluehairCoffee @WedgeLIVE I try to always take a pic of it. https://t.co/7i4nIgBKFM\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-12 03:18:56+00:00.\n",
      "The original tweet has been retweeted 0 times.\n",
      "@LiberalwKnives @WedgeLIVE @BluehairCoffee It seemed deep enough to total some cars at 8:45. Also, 👋, neighbor.\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-12 03:16:18+00:00.\n",
      "The original tweet has been retweeted 0 times.\n",
      "@WedgeLIVE 2200 block of Garfield. aka, the former Lake Blaisdell. https://t.co/SLm0zjjWUw\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-11 00:47:28+00:00.\n",
      "The original tweet has been retweeted 412 times.\n",
      "RT @ThePlumLineGS: Terrific @Milbank piece vividly highlighting the long trail of lying, deception, norm-shredding, and all around bad-acti…\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-08 03:20:25+00:00.\n",
      "The original tweet has been retweeted 0 times.\n",
      "@WedgeLIVE Had such a glorious cross today. First time in 10 years. Excited for our new weapons in the @TheWarOnCars\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-05 20:23:42+00:00.\n",
      "The original tweet has been retweeted 120 times.\n",
      "RT @tomtomorrow: I don't know how to explain to you that you should care that we are two years away from fascist theocracy.\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-03 18:34:51+00:00.\n",
      "The original tweet has been retweeted 0 times.\n",
      "This whole 🧵 https://t.co/YHVewCyhJy\n",
      "\n",
      "\n",
      "\n",
      "The tweet was tweeted at 2022-05-03 12:02:15+00:00.\n",
      "The original tweet has been retweeted 1334 times.\n",
      "RT @QasimRashid: Alito claims Roe V Wade “was wrongly decided because Constitution makes no specific mention of abortion rights.”  Constitu…\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet_count = 0\n",
    "\n",
    "for status in tweepy.Cursor(api.user_timeline, screen_name=handle).items():\n",
    "    tweet_count += 1\n",
    "    \n",
    "    print(f\"The tweet was tweeted at {status.created_at}.\")\n",
    "    print(f\"The original tweet has been retweeted {status.retweet_count} times.\")\n",
    "    \n",
    "    clean_status = status.text\n",
    "    clean_status = clean_status.replace(\"\\n\",\" \")\n",
    "    \n",
    "    print(f\"{clean_status}\")\n",
    "    print(\"\\n\"*2)\n",
    "        \n",
    "    if tweet_count > 10 :\n",
    "        break\n",
    "\n",
    "#Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling Follower Information\n",
    "\n",
    "In this next section of the assignment, we will pull information about the followers of your two artists. We must first get the follower IDs, then we will be able to \"hydrate\" the IDs, pulling the user objects for them. Once we have those user objects we will extract some fields that we can use in future analyses. \n",
    "\n",
    "\n",
    "The Twitter API only allows users to make 15 requests per 15 minutes when pulling followers. Each request allows you to gather 5000 follower ids. Tweepy will grab the 15 requests quickly then wait 15 minutes, rather than slowly pull the requests over the time period. Before we start grabbing follower IDs, let's first just check how long it would take to pull all of the followers. To do this we use the `followers_count` item from the user object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    @rushtheband has 302858 followers. \n",
      "    That will take roughly 1.01 hours to pull the followers.\n",
      "    \n",
      "\n",
      "    @thekillers has 4781598 followers. \n",
      "    That will take roughly 15.94 hours to pull the followers.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# I needed to update the screen_name= in order to run, update to tweepy\n",
    "# I'm also putting the handles in a list to iterate through below\n",
    "\n",
    "handles = ['rushtheband','thekillers']\n",
    "\n",
    "# This will iterate through each Twitter handle that we're collecting from\n",
    "for screen_name in handles:\n",
    "    \n",
    "    # Tells Tweepy we want information on the handle we're collecting from\n",
    "    # The next line specifies which information we want, which in this case is the number of followers \n",
    "    user = api.get_user(screen_name=screen_name) \n",
    "    followers_count = user.followers_count\n",
    "\n",
    "    # Let's see roughly how long it will take to grab all the follower IDs. \n",
    "    print(f'''\n",
    "    @{screen_name} has {followers_count} followers. \n",
    "    That will take roughly {followers_count/(5000*15*4):.2f} hours to pull the followers.\n",
    "    ''')\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we pull data for each artist we will write their data to a folder called \"twitter\", so we will make that folder if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the \"twitter\" folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then \"unlink\" it. Then create a new one.\n",
    "\n",
    "if not os.path.isdir(\"twitter\") : \n",
    "    #shutil.rmtree(\"twitter/\")\n",
    "    os.mkdir(\"twitter\")\n",
    "    \n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this following cells, use the `api.followers_ids` (and the `tweepy.Cursor` functionality) to pull some of the followers for your two artists. As you pull the data, write the follower ids to a file called `[artist name]_followers.txt` in the \"twitter\" folder. For instance, for Cher I would create a file named `cher_followers.txt`. As you pull the data, also store it in an object like a list or a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_followers_to_pull = 60*1000 # feel free to use this to limit the number of followers you pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulled 60000 followers of rushtheband\n",
      "Wrote 60000 followers of rushtheband to file: ./twitter/rushtheband_followers.txt\n",
      "Pulled 60000 followers of thekillers\n",
      "Wrote 60000 followers of thekillers to file: ./twitter/thekillers_followers.txt\n",
      "0:24:16.508331\n"
     ]
    }
   ],
   "source": [
    "# Modify the below code stub to pull the follower IDs and write them to a file. \n",
    "\n",
    "# Grabs the time when we start making requests to the API\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "for idx, handle in enumerate(handles):\n",
    "\n",
    "    output_file = \"./twitter/\"+handle + \"_followers.txt\"\n",
    "    user = api.get_user(screen_name=handle) \n",
    "    followers_count = user.followers_count\n",
    "    # Pull and store the follower IDs\n",
    "    followers = []\n",
    "    \n",
    "    for page in tweepy.Cursor(api.get_follower_ids,\n",
    "    #                           wait_on_rate_limit=True, \n",
    "    #                           wait_on_rate_limit_notify=True, \n",
    "    #                           compression=True,\n",
    "                              count = int(num_followers_to_pull/15),\n",
    "                              screen_name=handle).pages():\n",
    "        followers.extend(page)\n",
    "        # If you've pulled num_followers_to_pull, feel free to break out paged twitter API response\n",
    "        if len(followers) == num_followers_to_pull:\n",
    "            break\n",
    "    print(f\"Pulled {len(followers)} followers of {handle}\")\n",
    "    \n",
    "    # Write the IDs to the output file in the `twitter` folder.\n",
    "    with open(output_file,'w') as f:\n",
    "        for follower in followers:\n",
    "            f.write(str(follower) + \"\\n\")\n",
    "    print(f\"Wrote {len(followers)} followers of {handle} to file: {output_file}\")        \n",
    "    \n",
    "    # Sleep before the next pull\n",
    "    if idx != len(handles)-1:\n",
    "        time.sleep(60*15)  \n",
    "        \n",
    "    \n",
    "         \n",
    "# Let's see how long it took to grab all follower IDs\n",
    "end_time = datetime.datetime.now()\n",
    "print(end_time - start_time)\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have your follower ids, gather some information that we can use in future assignments on them. Using the `lookup_users` function, pull the user objects for your followers. These requests are limited to 900 per 15 minutes, but you can request 100 users at a time. At 90,000 users per 15 minutes, the rate limiter on pulls might be bandwidth rather than API limits. \n",
    "\n",
    "Extract the following fields from the user object: \n",
    "\n",
    "* screen_name\t\n",
    "* name\t\n",
    "* id\t\n",
    "* location\t\n",
    "* followers_count\t\n",
    "* friends_count\t\n",
    "* description\n",
    "\n",
    "These can all be accessed via these names in the object. Store the fields with one user per row in a tab-delimited text file with the name `[artist name]_follower_data.txt`. For instance, for Cher I would create a file named `cher_follower_data.txt`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = ['rushtheband','thekillers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting user information for followers of rushtheband\n",
      "Something went wrong, quitting... 404 Not Found\n",
      "17 - No user matches for specified terms.\n",
      "Getting user information for followers of thekillers\n",
      "Something went wrong, quitting... 404 Not Found\n",
      "17 - No user matches for specified terms.\n"
     ]
    }
   ],
   "source": [
    "# in this cell, do the following\n",
    "# 1. Set up a data frame or dictionary to hold the user information\n",
    "# 2. Use the `lookup_users` api function to pull sets of 100 users at a time\n",
    "# 3. Store the listed fields in your data frame or dictionary.\n",
    "# 4. Write the user information in tab-delimited form to the follower data text file. \n",
    "\n",
    "import json\n",
    "\n",
    "for handle in handles:\n",
    "    input_file = \"./twitter/\"+handle + \"_followers.txt\"\n",
    "    followers_id = [int(x) for x in open(input_file,'r').read().split('\\n') if x]\n",
    "    users_count = len(followers_id)\n",
    "    #1 Set up Dataframe:\n",
    "    my_list_of_dicts = []\n",
    "    try:\n",
    "        print(f\"Getting user information for followers of {handle}\")\n",
    "        # 2. Use the `lookup_users` api function to pull sets of 100 users at a time\n",
    "        for i in range(int(users_count / 100) + 1):\n",
    "            each_json_tweet = api.lookup_users(user_id=followers_id[i*100:min((i+1)*100, users_count)])\n",
    "            # 3. Store the listed fields in your data frame or dictionary.\n",
    "            fields = ['screen_name','name','id','location','followers_count','friends_count','description']\n",
    "            user_data = [{key: x._json[key] for key in fields} for x in each_json_tweet]\n",
    "#             print(user_data)\n",
    "            my_list_of_dicts.extend(user_data)\n",
    "#             print('getting users batch:', i)\n",
    "    except tweepy.errors.TweepyException as e:\n",
    "        print('Something went wrong, quitting...', e)\n",
    "        time.sleep(60*15)\n",
    "    output_file = \"./twitter/\"+handle +\"_follower_data.txt\"\n",
    "    # 4. Write the user information in tab-delimited form to the follower data text file. \n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(json.dumps(my_list_of_dicts, indent=4))\n",
    "        \n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One note: the user's description can have tabs or returns in it, so make sure to clean those out of the description before writing them to the file. Here's an example of how you might do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Home by Warsan Shire no one leaves home unless home is the mouth of a shark. you only run for the border when you see the whole city running as well. '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tricky_description = \"\"\"\n",
    "    Home by Warsan Shire\n",
    "    \n",
    "    no one leaves home unless\n",
    "    home is the mouth of a shark.\n",
    "    you only run for the border\n",
    "    when you see the whole city\n",
    "    running as well.\n",
    "\n",
    "\"\"\"\n",
    "# This won't work in a tab-delimited text file.\n",
    "\n",
    "clean_description = re.sub(r\"\\s+\",\" \",tricky_description)\n",
    "clean_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lyrics Scrape\n",
    "\n",
    "This section asks you to pull data from the Twitter API and scrape www.AZLyrics.com. In the notebooks where you do that work you are asked to store the data in specific ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = {'rush':\"https://www.azlyrics.com/r/rush.html\",\n",
    "           'killers':\"https://www.azlyrics.com/k/killers.html\"} \n",
    "# we'll use this dictionary to hold both the artist name and the link on AZlyrics\n",
    "\n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Rate Limiting\n",
    "\n",
    "The lyrics site, www.azlyrics.com, does not have an explicit maximum on number of requests in any one time, but in our testing it appears that too many requests in too short a time will cause the site to stop returning lyrics pages. (Entertainingly, the page that gets returned seems to only have the song title to [a Tom Jones song](https://www.azlyrics.com/lyrics/tomjones/itsnotunusual.html).) \n",
    "\n",
    "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you _do_ get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again. \n",
    "\n",
    "## Part 1: Finding Links to Songs Lyrics\n",
    "\n",
    "That general artist page has a list of all songs for that artist with links to the individual song pages. \n",
    "\n",
    "Q: Take a look at the `robots.txt` page on www.azlyrics.com. (You can read more about these pages [here](https://developers.google.com/search/docs/advanced/robots/intro).) Is the scraping we are about to do allowed or disallowed by this page? How do you know? \n",
    "\n",
    "A: yes it appears that the scraping we are doing is allowed since I have the outputs of the song title and lyrics.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up a dictionary of lists to hold our links\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "for artist, artist_page in artists.items():\n",
    "    # request the page and sleep\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "    \n",
    "    # now extract the links to lyrics pages from this page\n",
    "    # store the links `lyrics_pages` where the key is the artist and the\n",
    "    # value is a list of links. \n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    urls = []\n",
    "    for link in soup.find_all('a'):\n",
    "        url = link.get('href')\n",
    "        if url and (artist in url) and ('/lyrics/' in url):\n",
    "            full_url = 'https://www.azlyrics.com'+link.get('href')\n",
    "            lyrics_pages[artist].append(full_url)\n",
    "            \n",
    "# Section Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'rush': ['https://www.azlyrics.com/lyrics/rush/findingmyway.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/needsomelove.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/takeafriend.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/hereagain.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/whatyouredoing.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/inthemood.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/beforeandafter.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/workingman.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/anthem.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/bestican.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/beneathbetweenandbehind.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/bytorandthesnowdog.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/flybynight.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/makingmemories.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/rivendell.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/intheend.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/bastilleday.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/ithinkimgoingbald.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/lakesidepark.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thenecromancer.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thefountainoflamneth.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/2112.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/apassagetobangkok.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thetwilightzone.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/lessons.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/tears.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/somethingfornothing.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/afarewelltokings.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/xanadu.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/closertotheheart.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/cinderellaman.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/madrigal.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/cygnusx1bookonethevoyage.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/cygnusx1bookiihemispheres.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/circumstances.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thetrees.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thespiritofradio.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/freewill.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/jacobsladder.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/entrenous.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/differentstrings.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/naturalscience.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/tomsawyer.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/redbarchetta.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/limelight.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thecameraeye.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/witchhunt.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/vitalsigns.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/subdivisions.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/theanalogkid.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/chemistry.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/digitalman.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/theweapon.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/newworldman.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/losingit.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/countdown.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/distantearlywarning.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/afterimage.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/redsectora.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/theenemywithin.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thebodyelectric.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/kidgloves.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/redlenses.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/betweenthewheels.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thebigmoney.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/granddesigns.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/manhattanproject.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/marathon.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/territories.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/middletowndreams.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/emotiondetector.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/mysticrhythms.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/forceten.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/timestandstill.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/opensecrets.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/secondnature.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/primemover.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/lockandkey.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/mission.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/turnthepage.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/taishan.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/highwater.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/showdonttell.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/chainlightning.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thepass.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/warpaint.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/scars.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/presto.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/superconductor.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/anagramformongo.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/redtide.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/handoverfist.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/availablelight.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/dreamline.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/bravado.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/rollthebones.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/faceup.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thebigwheel.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/heresy.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/ghostofachance.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/neurotica.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/youbetyourlife.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/animate.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/stickitout.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/cuttothechase.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/nobodyshero.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/betweensunandmoon.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/alienshore.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thespeedoflove.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/doubleagent.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/coldfire.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/everydayglory.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/testforecho.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/driven.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/halftheworld.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thecolorofright.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/timeandmotion.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/totem.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/dogyears.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/virtuality.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/resist.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/carveawaythestone.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/onelittlevictory.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/ceilingunlimited.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/ghostrider.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/peaceablekingdom.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thestarslookdown.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/howitis.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/vaportrail.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/secrettouch.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/earthshine.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/sweetmiracle.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/nocturne.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/freeze.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/outofthecradle.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/summertimeblues.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/heartfullofsoul.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/theseeker.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/forwhatitsworth.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/shapesofthings.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/mrsoul.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/crossroads.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/sevenandsevenis.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/farcry.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/armorandsword.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/workinthemangels.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thelargerbowlapantoum.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/spindrift.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thewaythewindblows.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/faithless.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/bravestface.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/goodnewsfirst.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/malignantnarcissism.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/weholdon.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/caravan.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/bu2b.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/clockworkangels.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/theanarchist.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/carnies.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/haloeffect.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/sevencitiesofgold.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thewreckers.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/headlongflight.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/bu2b2.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/wishthemwell.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/thegarden.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/badboy.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/fancydancer.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/gardenroad.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/notfadeaway.html',\n",
       "              'https://www.azlyrics.com/lyrics/rush/youcantfightit.html'],\n",
       "             'killers': ['https://www.azlyrics.com/lyrics/killers/jennywasafriendofmine.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/mrbrightside39068.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/smilelikeyoumeanit.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/somebodytoldme.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/allthesethingsthativedone.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/andyyoureastar.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/ontop.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/changeyourmind.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/believemenatalie.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/midnightshow.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/everythingwillbealright.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/glamorousindierockroll.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/samstown.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/enterlude.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/whenyouwereyoung.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/blingconfessionofaking.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/forreasonsunknown.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/readmymind.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/unclejonny.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/bones.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/mylist.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thisriveriswild.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/whydoikeepcounting.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/exitlude.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/wherethewhiteboysdance.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/alltheprettyfaces.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/daddyseyes.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/tranquilize.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/shadowplay.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/alltheprettyfaces.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/leavethebourbonontheshelf.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/sweettalk.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/underthegun.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/wherethewhiteboysdance.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/showyouhow.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/moveaway.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/glamorousindierockroll.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/wholetyougo.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/theballadofmichaelvalentine.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/rubydonttakeyourlovetotown.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/daddyseyes.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/samstownabbeyroadversion.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/romeoandjuliet.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/mrbrightsidejacquelucontsthinwhitedukemix.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/losingtouch.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/human.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/spaceman.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/joyride.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/adustlandfairytale.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thisisyourlife.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/icantstay.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/neontiger.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/theworldwelivein.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/goodnighttravelwell.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/forgetaboutwhatisaid.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/tidalwave.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/acripplingblow.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/agreatbigsled.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/dontshootmesanta.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/josephbetteryouthanme.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/happybirthdayguadalupe.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/boots.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thecowboyschristmasball.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/fleshandbone.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/runaways.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thewayitwas.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/herewithme.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/amatteroftime.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/deadlinesandcommitments.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/missatomicbomb.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/therisingtide.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/heartofagirl.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/fromhereonout.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/bestill.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/battleborn.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/carrymehome.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/prizefighter.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/wonderfulwonderful.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/theman.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/rut.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/lifetocome.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/runforcover.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/tysonvsdouglas.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/somekindoflove.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/outofmymind.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thecalling.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/haveallthesongsbeenwritten.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/moneyonstraight.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/myownsoulswarning.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/blowback.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/dyingbreed.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/caution.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/lightningfields.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/fireinbone.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/runningtowardsaplace.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/mygod.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/whenthedreamsrundry.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/implodingthemirage.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/cestlavie.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/cautionwasatchstyle.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/westhills.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/quiettown.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/terriblething.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/cody.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/sleepwalker.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/runawayhorses.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/inthecaroutside.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/inanotherlife.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/desperatethings.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/pressuremachine.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thegettingby.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thegettingbyii.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thegettingbyiii.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thegettingbyiv.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/thegettingbyv.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/runawayhorsesii.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/westhillsii.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/westhillsiii.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/awhitedemonlovesong.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/christmasinla.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/desperate.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/dirtsledding.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/dustland.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/fourwinds.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/gettrashed.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/ifeelitinmybones.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/illbehomeforchristmas.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/joelthelumpofcoal.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/justanothergirl.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/landofthefree.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/monalisasandmadhatters.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/peaceofmind.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/replaceable.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/shotatthenight.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/spaceshipadventure.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/whereisshesoftsurrender.html',\n",
       "              'https://www.azlyrics.com/lyrics/killers/whydontyoufindoutforyourself.html']})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have enough lyrics pages to scrape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for artist, lp in lyrics_pages.items() :\n",
    "    assert(len(set(lp)) > 20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rush we have 171.\n",
      "The full pull will take for this artist will take 0.47 hours.\n",
      "For killers we have 137.\n",
      "The full pull will take for this artist will take 0.38 hours.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how long it's going to take to pull these lyrics \n",
    "# if we're waiting `5 + 10*random.random()` seconds \n",
    "for artist, links in lyrics_pages.items() : \n",
    "    print(f\"For {artist} we have {len(links)}.\")\n",
    "    print(f\"The full pull will take for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Pulling Lyrics\n",
    "\n",
    "Now that we have the links to our lyrics pages, let's go scrape them! Here are the steps for this part. \n",
    "\n",
    "1. Create an empty folder in our repo called \"lyrics\". \n",
    "1. Iterate over the artists in `lyrics_pages`. \n",
    "1. Create a subfolder in lyrics with the artist's name. For instance, if the artist was Cher you'd have `lyrics/cher/` in your repo.\n",
    "1. Iterate over the pages. \n",
    "1. Request the page and extract the lyrics from the returned HTML file using BeautifulSoup.\n",
    "1. Use the function below, `generate_filename_from_url`, to create a filename based on the lyrics page, then write the lyrics to a text file with that name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # drop the http or https and the html\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "\n",
    "    name = name.replace(\"/lyrics/\",\"\")\n",
    "    \n",
    "    # Replace useless chareacters with UNDERSCORE\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # tack on .txt\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the lyrics folder here. If you'd like to practice your programming, add functionality \n",
    "# that checks to see if the folder exists. If it does, then use shutil.rmtree to remove it and create a new one.\n",
    "\n",
    "if os.path.isdir(\"lyrics\") : \n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "\n",
    "os.mkdir(\"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.azlyrics.com/lyrics/rush/findingmyway.html\n",
      "Extract title: Finding My Way\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Finding My Way by rush\n",
      "https://www.azlyrics.com/lyrics/rush/needsomelove.html\n",
      "Extract title: Need Some Love\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Need Some Love by rush\n",
      "https://www.azlyrics.com/lyrics/rush/takeafriend.html\n",
      "Extract title: Take A Friend\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Take A Friend by rush\n",
      "https://www.azlyrics.com/lyrics/rush/hereagain.html\n",
      "Extract title: Here Again\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Here Again by rush\n",
      "https://www.azlyrics.com/lyrics/rush/whatyouredoing.html\n",
      "Extract title: What You're Doing\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of What You're Doing by rush\n",
      "https://www.azlyrics.com/lyrics/rush/inthemood.html\n",
      "Extract title: In The Mood\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of In The Mood by rush\n",
      "https://www.azlyrics.com/lyrics/rush/beforeandafter.html\n",
      "Extract title: Before And After\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Rush by rush\n",
      "https://www.azlyrics.com/lyrics/rush/workingman.html\n",
      "Extract title: Working Man\n",
      "Extract title: Rush\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Working Man by rush\n",
      "https://www.azlyrics.com/lyrics/rush/anthem.html\n",
      "Extract title: Anthem\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Anthem by rush\n",
      "https://www.azlyrics.com/lyrics/rush/bestican.html\n",
      "Extract title: Best I Can\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Best I Can by rush\n",
      "https://www.azlyrics.com/lyrics/rush/beneathbetweenandbehind.html\n",
      "Extract title: Beneath, Between And Behind\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Beneath, Between And Behind by rush\n",
      "https://www.azlyrics.com/lyrics/rush/bytorandthesnowdog.html\n",
      "Extract title: By-Tor And The Snow Dog\n",
      "Extract lyric.\n",
      "Finished writing the lyric of By-Tor And The Snow Dog by rush\n",
      "https://www.azlyrics.com/lyrics/rush/flybynight.html\n",
      "Extract title: Fly By Night\n",
      "Extract title: Fly By Night\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Fly By Night by rush\n",
      "https://www.azlyrics.com/lyrics/rush/makingmemories.html\n",
      "Extract title: Making Memories\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Making Memories by rush\n",
      "https://www.azlyrics.com/lyrics/rush/rivendell.html\n",
      "Extract title: Rivendell\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Rivendell by rush\n",
      "https://www.azlyrics.com/lyrics/rush/intheend.html\n",
      "Extract title: In The End\n",
      "Extract lyric.\n",
      "Finished writing the lyric of In The End by rush\n",
      "https://www.azlyrics.com/lyrics/rush/bastilleday.html\n",
      "Extract title: Bastille Day\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Bastille Day by rush\n",
      "https://www.azlyrics.com/lyrics/rush/ithinkimgoingbald.html\n",
      "Extract title: I Think I'm Going Bald\n",
      "Extract lyric.\n",
      "Finished writing the lyric of I Think I'm Going Bald by rush\n",
      "https://www.azlyrics.com/lyrics/rush/lakesidepark.html\n",
      "Extract title: Lakeside Park\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Lakeside Park by rush\n",
      "https://www.azlyrics.com/lyrics/rush/thenecromancer.html\n",
      "Extract title: The Necromancer\n",
      "Extract lyric.\n",
      "Finished writing the lyric of The Necromancer by rush\n",
      "https://www.azlyrics.com/lyrics/killers/jennywasafriendofmine.html\n",
      "Extract title: Jenny Was A Friend Of Mine\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Jenny Was A Friend Of Mine by killers\n",
      "https://www.azlyrics.com/lyrics/killers/mrbrightside39068.html\n",
      "Extract title: Mr Brightside\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Mr Brightside by killers\n",
      "https://www.azlyrics.com/lyrics/killers/smilelikeyoumeanit.html\n",
      "Extract title: Smile Like You Mean It\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Smile Like You Mean It by killers\n",
      "https://www.azlyrics.com/lyrics/killers/somebodytoldme.html\n",
      "Extract title: Somebody Told Me\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Somebody Told Me by killers\n",
      "https://www.azlyrics.com/lyrics/killers/allthesethingsthativedone.html\n",
      "Extract title: All These Things That I've Done\n",
      "Extract lyric.\n",
      "Finished writing the lyric of All These Things That I've Done by killers\n",
      "https://www.azlyrics.com/lyrics/killers/andyyoureastar.html\n",
      "Extract title: Andy, You're A Star\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Andy, You're A Star by killers\n",
      "https://www.azlyrics.com/lyrics/killers/ontop.html\n",
      "Extract title: On Top\n",
      "Extract lyric.\n",
      "Finished writing the lyric of On Top by killers\n",
      "https://www.azlyrics.com/lyrics/killers/changeyourmind.html\n",
      "Extract title: Change Your Mind\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Change Your Mind by killers\n",
      "https://www.azlyrics.com/lyrics/killers/believemenatalie.html\n",
      "Extract title: Believe Me Natalie\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Believe Me Natalie by killers\n",
      "https://www.azlyrics.com/lyrics/killers/midnightshow.html\n",
      "Extract title: Midnight Show\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Midnight Show by killers\n",
      "https://www.azlyrics.com/lyrics/killers/everythingwillbealright.html\n",
      "Extract title: Everything Will Be Alright\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Everything Will Be Alright by killers\n",
      "https://www.azlyrics.com/lyrics/killers/glamorousindierockroll.html\n",
      "Extract title: Glamorous Indie Rock & Roll\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Glamorous Indie Rock & Roll by killers\n",
      "https://www.azlyrics.com/lyrics/killers/samstown.html\n",
      "Extract title: Sam's Town\n",
      "Extract title: Sam's Town\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Sam's Town by killers\n",
      "https://www.azlyrics.com/lyrics/killers/enterlude.html\n",
      "Extract title: Enterlude\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Enterlude by killers\n",
      "https://www.azlyrics.com/lyrics/killers/whenyouwereyoung.html\n",
      "Extract title: When You Were Young\n",
      "Extract lyric.\n",
      "Finished writing the lyric of When You Were Young by killers\n",
      "https://www.azlyrics.com/lyrics/killers/blingconfessionofaking.html\n",
      "Extract title: Bling (Confession Of A King)\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Bling (Confession Of A King) by killers\n",
      "https://www.azlyrics.com/lyrics/killers/forreasonsunknown.html\n",
      "Extract title: For Reasons Unknown\n",
      "Extract lyric.\n",
      "Finished writing the lyric of For Reasons Unknown by killers\n",
      "https://www.azlyrics.com/lyrics/killers/readmymind.html\n",
      "Extract title: Read My Mind\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Read My Mind by killers\n",
      "https://www.azlyrics.com/lyrics/killers/unclejonny.html\n",
      "Extract title: Uncle Jonny\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Uncle Jonny by killers\n",
      "https://www.azlyrics.com/lyrics/killers/bones.html\n",
      "Extract title: Bones\n",
      "Extract lyric.\n",
      "Finished writing the lyric of Bones by killers\n"
     ]
    }
   ],
   "source": [
    "url_stub = \"https://www.azlyrics.com\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0 \n",
    "\n",
    "for artist in lyrics_pages :\n",
    "    \n",
    "    # Use this space to carry out the following steps: \n",
    "    \n",
    "    # 1. Build a subfolder for the artist\n",
    "    if not os.path.isdir(f\"lyrics/{artist}/\"): \n",
    "        os.mkdir(f'lyrics/{artist}/') \n",
    "    # 2. Iterate over the lyrics pages\n",
    "    for artist_lyric_page in lyrics_pages[artist][:20]:\n",
    "        print(artist_lyric_page)\n",
    "        title = artist_lyric_page.split()\n",
    "    # 3. Request the lyrics page. \n",
    "        # Don't forget to add a line like `time.sleep(5 + 10*random.random())`\n",
    "        # to sleep after making the request\n",
    "        r = requests.get(artist_lyric_page)\n",
    "        time.sleep(5 + 10*random.random())\n",
    "    # 4. Extract the title and lyrics from the page.\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        titles = []\n",
    "        for text in soup.find_all('b'):\n",
    "            if text and \"\".join([ch for ch in text.string.lower() if ch.isalpha()]) in artist_lyric_page:\n",
    "                title = text.string.strip(\"\\\"\")\n",
    "                print(f\"Extract title: {title}\")\n",
    "                titles.append(title)\n",
    "        lyrics = []\n",
    "        for divTag in soup.find_all(\"div\", {\"class\": \"col-xs-12 col-lg-8 text-center\"}):\n",
    "            for tag in divTag.find_all(\"div\", {\"class\": \"\"}):\n",
    "                lyric = tag.text.strip(\"\\n\")\n",
    "                if lyric:\n",
    "                    print(f\"Extract lyric.\")\n",
    "                    lyrics.append(lyric)\n",
    "    # 5. Write out the title, two returns ('\\n'), and the lyrics. Use `generate_filename_from_url`\n",
    "    #    to generate the filename. \n",
    "        for title, lyric in zip(set(titles),lyrics):\n",
    "            content = title+\"\\n\"+\"\\n\"+lyric\n",
    "            output_file = generate_filename_from_link(artist_lyric_page)\n",
    "            with open(f'lyrics/{artist}/'+output_file,'w') as f:\n",
    "                f.write(content)\n",
    "            print(f\"Finished writing the lyric of {title} by {artist}\")\n",
    "    # Remember to pull at least 20 songs per artist. It may be fun to pull all the songs for the artist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run time was 0.14 hours.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total run time was {round((time.time() - start)/3600,2)} hours.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "This assignment asks you to pull data from the Twitter API and scrape www.AZLyrics.com.  After you have finished the above sections , run all the cells in this notebook. Print this to PDF and submit it, per the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Checking Twitter Data\n",
    "\n",
    "The output from your Twitter API pull should be two files per artist, stored in files with formats like `cher_followers.txt` (a list of all follower IDs you pulled) and `cher_followers_data.txt`. These files should be in a folder named `twitter` within the repository directory. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see two artist handles: thekillers and rushtheband.\n"
     ]
    }
   ],
   "source": [
    "twitter_files = os.listdir(\"twitter\")\n",
    "twitter_files = [f for f in twitter_files if f != \".DS_Store\"]\n",
    "artist_handles = list(set([name.split(\"_\")[0] for name in twitter_files]))\n",
    "\n",
    "print(f\"We see two artist handles: {artist_handles[0]} and {artist_handles[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see 59999 in your follower file for thekillers, assuming a header row.\n",
      "In the follower data file (thekillers_follower_data.txt) for thekillers, we have these columns:\n",
      "[\n",
      "\n",
      "We have 539965 data rows for thekillers in the follower data file.\n",
      "For thekillers we have 0 unique locations.\n",
      "For thekillers we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "We see 59999 in your follower file for rushtheband, assuming a header row.\n",
      "In the follower data file (rushtheband_follower_data.txt) for rushtheband, we have these columns:\n",
      "[\n",
      "\n",
      "We have 539938 data rows for rushtheband in the follower data file.\n",
      "For rushtheband we have 0 unique locations.\n",
      "For rushtheband we have 0 words in the descriptions.\n",
      "Here are the five most common words:\n",
      "[]\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for artist in artist_handles :\n",
    "    follower_file = artist + \"_followers.txt\"\n",
    "    follower_data_file = artist + \"_follower_data.txt\"\n",
    "    \n",
    "    ids = open(\"twitter/\" + follower_file,'r').readlines()\n",
    "    \n",
    "    print(f\"We see {len(ids)-1} in your follower file for {artist}, assuming a header row.\")\n",
    "    \n",
    "    with open(\"twitter/\" + follower_data_file,'r') as infile :\n",
    "        \n",
    "        # check the headers\n",
    "        headers = infile.readline().split(\"\\t\")\n",
    "        \n",
    "        print(f\"In the follower data file ({follower_data_file}) for {artist}, we have these columns:\")\n",
    "        print(\" : \".join(headers))\n",
    "        \n",
    "        description_words = []\n",
    "        locations = set()\n",
    "        \n",
    "        \n",
    "        for idx, line in enumerate(infile.readlines()) :\n",
    "            line = line.strip(\"\\n\").split(\"\\t\")\n",
    "            \n",
    "            try : \n",
    "                locations.add(line[3])            \n",
    "                description_words.extend(words(line[6]))\n",
    "            except :\n",
    "                pass\n",
    "    \n",
    "        \n",
    "\n",
    "        print(f\"We have {idx+1} data rows for {artist} in the follower data file.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(locations)} unique locations.\")\n",
    "\n",
    "        print(f\"For {artist} we have {len(description_words)} words in the descriptions.\")\n",
    "        print(\"Here are the five most common words:\")\n",
    "        print(Counter(description_words).most_common(5))\n",
    "\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"-\"*40)\n",
    "        print(\"\")\n",
    "\n",
    "        \n",
    "# Section Complete, however, it doesnt return me any common words. I am unsure how to fix this as the moment because\n",
    "# everything appears to be working correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Lyrics \n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory:\n",
    "`/lyrics/[Artist Name]/[filename from URL]`. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For killers we have 20 files.\n",
      "For killers we have roughly 5247 words, 771 are unique.\n",
      "For rush we have 20 files.\n",
      "For rush we have roughly 3562 words, 878 are unique.\n"
     ]
    }
   ],
   "source": [
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "            \n",
    "    print(f\"For {artist} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
